{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"\"\"\"I love natural language processing.\"\n",
    "    \"BERT is a transformer model.\"\n",
    "    \"Tokenization is an essential step in NLP.\"\n",
    "    \"Deep learning models can understand text.\"\n",
    "    \"Hugging Face provides pre-trained models.\"\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"I love natural language processing.\"\n",
      "    \"BERT is a transformer model.\"\n",
      "    \"Tokenization is an essential step in NLP.\"\n",
      "    \"Deep learning models can understand text.\"\n",
      "    \"Hugging Face provides pre-trained models.\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert CORPUS (paragraphs) to SENTENCES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize,word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"I love natural language processing.\"',\n",
       " '\"BERT is a transformer model.\"',\n",
       " '\"Tokenization is an essential step in NLP.\"',\n",
       " '\"Deep learning models can understand text.\"',\n",
       " '\"Hugging Face provides pre-trained models.\"']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = sent_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TIME TO TOKENIZE WORDS FROM CORPUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tokens = word_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['``', 'I', 'love', 'natural', 'language', 'processing', '.', \"''\"]\n",
      "['``', 'BERT', 'is', 'a', 'transformer', 'model', '.', \"''\"]\n",
      "['``', 'Tokenization', 'is', 'an', 'essential', 'step', 'in', 'NLP', '.', \"''\"]\n",
      "['``', 'Deep', 'learning', 'models', 'can', 'understand', 'text', '.', \"''\"]\n",
      "['``', 'Hugging', 'Face', 'provides', 'pre-trained', 'models', '.', \"''\"]\n"
     ]
    }
   ],
   "source": [
    "sentence_tokens = [word_tokenize(sentence) for sentence in sentences]\n",
    "for i in sentence_tokens:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"',\n",
       " 'I',\n",
       " 'love',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " '.\"',\n",
       " '\"',\n",
       " 'BERT',\n",
       " 'is',\n",
       " 'a',\n",
       " 'transformer',\n",
       " 'model',\n",
       " '.\"',\n",
       " '\"',\n",
       " 'Tokenization',\n",
       " 'is',\n",
       " 'an',\n",
       " 'essential',\n",
       " 'step',\n",
       " 'in',\n",
       " 'NLP',\n",
       " '.\"',\n",
       " '\"',\n",
       " 'Deep',\n",
       " 'learning',\n",
       " 'models',\n",
       " 'can',\n",
       " 'understand',\n",
       " 'text',\n",
       " '.\"',\n",
       " '\"',\n",
       " 'Hugging',\n",
       " 'Face',\n",
       " 'provides',\n",
       " 'pre',\n",
       " '-',\n",
       " 'trained',\n",
       " 'models',\n",
       " '.\"']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "\n",
    "wordpunct_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['``',\n",
       " 'I',\n",
       " 'love',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing.',\n",
       " \"''\",\n",
       " '``',\n",
       " 'BERT',\n",
       " 'is',\n",
       " 'a',\n",
       " 'transformer',\n",
       " 'model.',\n",
       " \"''\",\n",
       " '``',\n",
       " 'Tokenization',\n",
       " 'is',\n",
       " 'an',\n",
       " 'essential',\n",
       " 'step',\n",
       " 'in',\n",
       " 'NLP.',\n",
       " \"''\",\n",
       " '``',\n",
       " 'Deep',\n",
       " 'learning',\n",
       " 'models',\n",
       " 'can',\n",
       " 'understand',\n",
       " 'text.',\n",
       " \"''\",\n",
       " '``',\n",
       " 'Hugging',\n",
       " 'Face',\n",
       " 'provides',\n",
       " 'pre-trained',\n",
       " 'models',\n",
       " '.',\n",
       " \"''\"]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "tokenizer.tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PORTER STEMMER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "words = ['go','gone','going','eat','eats','eating','eaten','write','writing','written','programming','programs','program','history']\n",
    "\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go -------> go\n",
      "gone -------> gone\n",
      "going -------> go\n",
      "eat -------> eat\n",
      "eats -------> eat\n",
      "eating -------> eat\n",
      "eaten -------> eaten\n",
      "write -------> write\n",
      "writing -------> write\n",
      "written -------> written\n",
      "programming -------> program\n",
      "programs -------> program\n",
      "program -------> program\n",
      "history -------> histori\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word+\" -------> \"+stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'congratul'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('congratulate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REGEXP STEMMER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import RegexpStemmer\n",
    "\n",
    "# pass a regular expression in it\n",
    "reg_stemmer = RegexpStemmer('ing$|s$|e$|able$',min=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go -------> go\n",
      "gone -------> gon\n",
      "going -------> go\n",
      "eat -------> eat\n",
      "eats -------> eat\n",
      "eating -------> eat\n",
      "eaten -------> eaten\n",
      "write -------> writ\n",
      "writing -------> writ\n",
      "written -------> written\n",
      "programming -------> programm\n",
      "programs -------> program\n",
      "program -------> program\n",
      "history -------> history\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word+\" -------> \"+reg_stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ingredient'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_stemmer.stem('ingredients')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SNOWBALL STEMMER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "snowball  = SnowballStemmer(language='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go -------> go\n",
      "gone -------> gone\n",
      "going -------> go\n",
      "eat -------> eat\n",
      "eats -------> eat\n",
      "eating -------> eat\n",
      "eaten -------> eaten\n",
      "write -------> write\n",
      "writing -------> write\n",
      "written -------> written\n",
      "programming -------> program\n",
      "programs -------> program\n",
      "program -------> program\n",
      "history -------> histori\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word+\" -------> \"+snowball.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('addict', 'sportingli')"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('addictive'),stemmer.stem('sportingly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('addict', 'sport')"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snowball.stem('addictive'),snowball.stem('sportingly')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LEMMATIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go -------> go\n",
      "gone -------> go\n",
      "going -------> going\n",
      "eat -------> eat\n",
      "eats -------> eats\n",
      "eating -------> eating\n",
      "eaten -------> eat\n",
      "write -------> write\n",
      "writing -------> writing\n",
      "written -------> write\n",
      "programming -------> programming\n",
      "programs -------> program\n",
      "program -------> program\n",
      "history -------> history\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word+\" -------> \"+lemmatizer.morphy(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
